<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>marcio.io</title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://marcio.io/blog/</link>
    <language>en-us</language>
    <author>Marcio Castilho</author>
    <copyright>2015 Marcio Castilho</copyright>
    <updated>Tue, 07 Jul 2015 11:14:06 EDT</updated>
    
    
    <item>
      <title>Calculating Multiple File Hashes in a Single Pass</title>
      <link>http://marcio.io/2015/07/calculating-multiple-file-hashes-in-a-single-pass/</link>
      <pubDate>Tue, 07 Jul 2015 11:14:06 EDT</pubDate>
      <author>Marcio Castilho</author>
      <guid>http://marcio.io/2015/07/calculating-multiple-file-hashes-in-a-single-pass/</guid>
      <description>

&lt;p&gt;We do a lot of file hash calculations at work, where we commonly go through millions of files a day using a diverse number of hashing algorithms. The standard Go library is amazing, and it has many structures and methods to do all that kind of stuff. Sometimes you just have to look for some new methods that opens up the possibilities even more.&lt;/p&gt;

&lt;h3 id=&#34;the-goal:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;The Goal&lt;/h3&gt;

&lt;p&gt;The initial goal of this code was to calculate multiple hashes on a single file residing on disk, and only perform a single read, instead of reading the whole contents of the file multiple times for each hash algorithm.&lt;/p&gt;

&lt;p&gt;The idea was to return an structure with the results of the desired hash:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type HashInfo struct {
	Md5    string `json:&amp;quot;md5&amp;quot;`
	Sha1   string `json:&amp;quot;sha1&amp;quot;`
	Sha256 string `json:&amp;quot;sha256&amp;quot;`
	Sha512 string `json:&amp;quot;sha512&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;looking-into-the-standard-library:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;Looking into the standard library&lt;/h3&gt;

&lt;p&gt;As park of the Go standard library &lt;code&gt;io&lt;/code&gt; package, we can find this function below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func MultiWriter(writers ...Writer) Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is a snippet of the source code implementation of this function in the &lt;code&gt;io&lt;/code&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type multiWriter struct {
    writers []Writer
}

func (t *multiWriter) Write(p []byte) (n int, err error) {
	for _, w := range t.writers {
		n, err = w.Write(p)
		if err != nil {
			return
		}
		if n != len(p) {
			err = ErrShortWrite
			return
		}
	}
	return len(p), nil
}

func MultiWriter(writers ...Writer) Writer {
	w := make([]Writer, len(writers))
	copy(w, writers)
	return &amp;amp;multiWriter{w}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The MultiWriter method creates a writer that duplicates its writes to all the provided writers, similar to the Unix &lt;code&gt;tee&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;This is interesting because since all of hash functions in standard library adheres to this interface:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Hash interface {
        // Write (via the embedded io.Writer interface) adds more data to the running hash.
        // It never returns an error.
        io.Writer

        // Sum appends the current hash to b and returns the resulting slice.
        // It does not change the underlying hash state.
        Sum(b []byte) []byte

        // Reset resets the Hash to its initial state.
        Reset()

        // Size returns the number of bytes Sum will return.
        Size() int

        // BlockSize returns the hash&#39;s underlying block size.
        // The Write method must be able to accept any amount
        // of data, but it may operate more efficiently if all writes
        // are a multiple of the block size.
        BlockSize() int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-approach:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;The Approach&lt;/h3&gt;

&lt;p&gt;Therefore, we could create a &lt;code&gt;MultiWriter&lt;/code&gt; that is going to write to multiple Hash implementations only performing a single read pass through the original file, as you can see in the code below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func CalculateBasicHashes(rd io.Reader) HashInfo {

	md5 := md5.New()
	sha1 := sha1.New()
	sha256 := sha256.New()
	sha512 := sha512.New()

	// For optimum speed, Getpagesize returns the underlying system&#39;s memory page size.
	pagesize := os.Getpagesize()

	// wraps the Reader object into a new buffered reader to read the files in chunks
	// and buffering them for performance.
	reader := bufio.NewReaderSize(rd, pagesize)

	// creates a multiplexer Writer object that will duplicate all write
	// operations when copying data from source into all different hashing algorithms
	// at the same time
	multiWriter := io.MultiWriter(md5, sha1, sha256, sha512)

	// Using a buffered reader, this will write to the writer multiplexer
	// so we only traverse through the file once, and can calculate all hashes
	// in a single byte buffered scan pass.
	//
	_, err := io.Copy(multiWriter, reader)
	if err != nil {
		panic(err.Error())
	}

	var info HashInfo

	info.Md5 = hex.EncodeToString(md5.Sum(nil))
	info.Sha1 = hex.EncodeToString(sha1.Sum(nil))
	info.Sha256 = hex.EncodeToString(sha256.Sum(nil))
	info.Sha512 = hex.EncodeToString(sha512.Sum(nil))

	return info
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is a sample of command line utility to calculate the multiple hashes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;bufio&amp;quot;
	&amp;quot;crypto/md5&amp;quot;
	&amp;quot;crypto/sha1&amp;quot;
	&amp;quot;crypto/sha256&amp;quot;
	&amp;quot;crypto/sha512&amp;quot;
	&amp;quot;encoding/hex&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;io&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;runtime&amp;quot;
)

func main() {
	args := os.Args[1:]

	var filename string
	filename = args[0]

    // open an io.Reader from the file we would like to calculate hashes
	f, err := os.OpenFile(filename, os.O_RDONLY, 0)
	if err != nil {
		log.Fatalln(&amp;quot;Cannot open file: %s&amp;quot;, filename)
	}
	defer f.Close()

	info := CalculateBasicHashes(f)

	fmt.Println(&amp;quot;md5    :&amp;quot;, info.Md5)
	fmt.Println(&amp;quot;sha1   :&amp;quot;, info.Sha1)
	fmt.Println(&amp;quot;sha256 :&amp;quot;, info.Sha256)
	fmt.Println(&amp;quot;sha512 :&amp;quot;, info.Sha512)
	fmt.Println()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course that in a real-world scenario we wouldn&amp;rsquo;t be invoking the command line utility for every single file. This was just a simple example on how to write a little command line utility to demonstrate this approach. The real benefit is when we are traversing through millions of files and performing hash calculations using a single read pass through the contents file. This has a significant impact on our ability to fast go through our file repositories.&lt;/p&gt;

&lt;p&gt;There are so many interesting functions and interfaces in the standard library that everyone should take look at the source code once in a while.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Handling 1 Million Requests per Minute with Go</title>
      <link>http://marcio.io/2015/07/handling-1-million-requests-per-minute-with-golang/</link>
      <pubDate>Mon, 06 Jul 2015 16:49:35 EDT</pubDate>
      <author>Marcio Castilho</author>
      <guid>http://marcio.io/2015/07/handling-1-million-requests-per-minute-with-golang/</guid>
      <description>

&lt;p&gt;Here at &lt;a href=&#34;http://www.malwarebytes.org&#34;&gt;Malwarebytes&lt;/a&gt; we are experiencing phenomenal growth, and since I have joined the company over 1 year ago in the Silicon Valley, one my main responsibilities has been to architect and develop several systems to power a fast-growing security company and all the needed infrastructure to support a product that is used by millions of people every single day. I have worked in the anti-virus and anti-malware industry for over 12 years in a few different companies, and I knew how complex these systems could end up being due to the massive amount of data we handle daily.&lt;/p&gt;

&lt;p&gt;What is interesting is that for the last 9 years or so, all the web backend development that I have been involved in has been mostly done in Ruby on Rails. Don&amp;rsquo;t take me wrong, I love Ruby on Rails and I believe it&amp;rsquo;s an amazing environment, but after a while you start thinking and designing systems in the ruby way, and you forget how efficient and simple your software architecture could have been if you could leverage multi-threading, parallelization, fast executions and small memory overhead. For many years, I was a C/C++, Delphi and C# developer, and I just started realizing how less complex things could be with the right tool for the job.&lt;/p&gt;

&lt;p&gt;As a Principal Architect, I am not very big on the language and framework wars that the interwebs are always fighting about. I believe efficiency, productivity and code maintainability relies mostly on how simple you can architect your solution.&lt;/p&gt;

&lt;h2 id=&#34;the-problem:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;While working on a piece of our anonymous telemetry and analytics system, our goal was to be able to handle a large amount of POST requests from millions of endpoints. The web handler would receive a JSON document that may contain a collection of many payloads that needed to be written to Amazon S3, in order for our map-reduce systems to later operate on this data.&lt;/p&gt;

&lt;p&gt;Traditionally we would look into creating a worker-tier architecture, utilizing things such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sidekiq&lt;/li&gt;
&lt;li&gt;Resque&lt;/li&gt;
&lt;li&gt;DelayedJob&lt;/li&gt;
&lt;li&gt;Elasticbeanstalk Worker Tier&lt;/li&gt;
&lt;li&gt;RabbitMQ&lt;/li&gt;
&lt;li&gt;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And setup 2 different clusters, one for the web front-end and another for the workers, so we can scale up the amount of background work we can handle.&lt;/p&gt;

&lt;p&gt;But since the beginning, our team knew that we should do this in Go because during the discussion phases we saw this could be potentially a very large traffic system. I have been using Go for about 2 years or so, and we had developed a few systems here at work but none that would get this amount of load.&lt;/p&gt;

&lt;p&gt;We started by creating a few structures to define the web request payload that we would be receiving through the POST calls, and a method to upload it into our S3 bucket.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type PayloadCollection struct {
	WindowsVersion  string    `json:&amp;quot;version&amp;quot;`
	Token           string    `json:&amp;quot;token&amp;quot;`
	Payloads        []Payload `json:&amp;quot;data&amp;quot;`
}

type Payload struct {
    // [redacted]
}

func (p *Payload) UploadToS3() error {
    // the storageFolder method ensures that there are no name collision in
    // case we get same timestamp in the key name
    storage_path := fmt.Sprintf(&amp;quot;%v/%v&amp;quot;, p.storageFolder, time.Now().UnixNano())

	bucket := S3Bucket

	b := new(bytes.Buffer)
	encodeErr := json.NewEncoder(b).Encode(payload)
	if encodeErr != nil {
		return encodeErr
	}

    // Everything we post to the S3 bucket should be marked &#39;private&#39;
    var acl = s3.Private
	var contentType = &amp;quot;application/octet-stream&amp;quot;

	return bucket.PutReader(storage_path, b, int64(b.Len()), contentType, acl, s3.Options{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;naive-approach-to-go-routines:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Naive approach to Go routines&lt;/h3&gt;

&lt;p&gt;Initially we took a very naive implementation of the POST handler, just trying to parallelize the job processing into a simple goroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func payloadHandler(w http.ResponseWriter, r *http.Request) {

    if r.Method != &amp;quot;POST&amp;quot; {
		w.WriteHeader(http.StatusMethodNotAllowed)
		return
	}

    // Read the body into a string for json decoding
	var content = &amp;amp;PayloadCollection{}
	err := json.NewDecoder(io.LimitReader(r.Body, MaxLength)).Decode(&amp;amp;content)
    if err != nil {
		w.Header().Set(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json; charset=UTF-8&amp;quot;)
		w.WriteHeader(http.StatusBadRequest)
		return
	}

    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {
        go payload.UploadToS3()   // &amp;lt;----- DON&#39;T DO THIS
    }

    w.WriteHeader(http.StatusOK)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For moderate loads, this could work for the majority of people, but this quickly proved to not work very well at a large scale. We were expecting a lot of requests but not in the order of magnitude we started seeing when we deployed the first version to production. We completely understimated the amount of traffic.&lt;/p&gt;

&lt;p&gt;The approach above is bad in several different ways. There is no way to control how many go routines we are spawning. And since we were getting 1 million POST requests per minute of course this code crashed and burned very quickly.&lt;/p&gt;

&lt;h3 id=&#34;trying-again:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Trying again&lt;/h3&gt;

&lt;p&gt;We needed to find a different way. Since the beginning we started discussing how we needed to keep the lifetime of the request handler very short and spawn processing in the background. Of course, this is what you must do in the Ruby on Rails world, otherwise you will block all the available worker web processors, whether you are using puma, unicorn, passenger (Let&amp;rsquo;s not get into the JRuby discussion please). Then we would have needed to leverage common solutions to do this, such as Resque, Sidekiq, SQS, etc. The list goes on since there are many ways of achieving this.&lt;/p&gt;

&lt;p&gt;So the second iteration was to create a buffered channel where we could queue up some jobs and upload them to S3, and since we could control the maximum number of items in our queue and we had plenty of RAM to queue up jobs in memory, we thought it would be okay to just buffer jobs in the channel queue.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var Queue chan Payload

func init() {
    Queue = make(chan Payload, MAX_QUEUE)
}

func payloadHandler(w http.ResponseWriter, r *http.Request) {
    ...
    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {
        Queue &amp;lt;- payload
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then to actually dequeue jobs and process them, we were using something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func StartProcessor() {
    for {
        select {
        case job := &amp;lt;-Queue:
            job.payload.UploadToS3()  // &amp;lt;-- STILL NOT GOOD
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be honest, I have no idea what we were thinking. This must have been a late night full of Red-Bulls. This approach didn&amp;rsquo;t buy us anything, we have traded flawed concurrency with a buffered queue that was simply postponing the problem. Our synchronous processor was only uploading one payload at a time to S3, and since the rate of incoming requests were much larger than the ability of the single processor to upload to S3, our buffered channel was quickly reaching its limit and blocking the request handler ability to queue more items.&lt;/p&gt;

&lt;p&gt;We were simply avoiding the problem and started a count-down to the death of our system eventually. Our latency rates kept increasing in a constant rate minutes after we deployed this flawed version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://marcio.io/img/cloudwatch-latency.png&#34; alt=&#34;cloudwatch-latency&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-better-solution:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Better Solution&lt;/h3&gt;

&lt;p&gt;We have decided to utilize a common pattern when using Go channels, in order to create a 2-tier channel system, one for queuing jobs and another to control how many workers operate on the JobQueue concurrently.&lt;/p&gt;

&lt;p&gt;The idea was to parallelize the uploads to S3 to a somewhat sustainable rate, one that would not cripple the machine nor start generating connections errors from S3. So we have opted for creating a Job/Worker pattern. For those that are familiar with Java, C#, etc, think about this as the Golang way of implementing a Worker Thread-Pool utilizing channels instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var (
	MaxWorker = os.Getenv(&amp;quot;MAX_WORKERS&amp;quot;)
	MaxQueue  = os.Getenv(&amp;quot;MAX_QUEUE&amp;quot;)
)

// Job represents the job to be run
type Job struct {
	Payload Payload
}

// A buffered channel that we can send work requests on.
var JobQueue chan Job

// Worker represents the worker that executes the job
type Worker struct {
	WorkerPool  chan chan Job
	JobChannel  chan Job
	quit    	chan bool
}

func NewWorker(workerPool chan chan Job) Worker {
	return Worker{
		WorkerPool: workerPool,
		JobChannel: make(chan Job),
		quit:       make(chan bool)}
}

// Start method starts the run loop for the worker, listening for a quit channel in
// case we need to stop it
func (w Worker) Start() {
	go func() {
		for {
			// register the current worker into the worker queue.
			w.WorkerPool &amp;lt;- w.JobChannel

			select {
			case job := &amp;lt;-w.JobChannel:
				// we have received a work request.
				if err := job.Payload.UploadToS3(); err != nil {
					log.Errorf(&amp;quot;Error uploading to S3: %s&amp;quot;, err.Error())
				}

			case &amp;lt;-w.quit:
				// we have received a signal to stop
				return
			}
		}
	}()
}

// Stop signals the worker to stop listening for work requests.
func (w Worker) Stop() {
	go func() {
		w.quit &amp;lt;- true
	}()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have modified our Web request handler to create an instance of &lt;code&gt;Job&lt;/code&gt; struct with the payload and send into the &lt;code&gt;JobQueue&lt;/code&gt; channel for the workers to pickup.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func payloadHandler(w http.ResponseWriter, r *http.Request) {

    if r.Method != &amp;quot;POST&amp;quot; {
		w.WriteHeader(http.StatusMethodNotAllowed)
		return
	}

    // Read the body into a string for json decoding
	var content = &amp;amp;PayloadCollection{}
	err := json.NewDecoder(io.LimitReader(r.Body, MaxLength)).Decode(&amp;amp;content)
    if err != nil {
		w.Header().Set(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json; charset=UTF-8&amp;quot;)
		w.WriteHeader(http.StatusBadRequest)
		return
	}

    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {

        // let&#39;s create a job with the payload
        work := Job{Payload: payload}

        // Push the work onto the queue.
        JobQueue &amp;lt;- work
    }

    w.WriteHeader(http.StatusOK)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During our web server initialization we create a &lt;code&gt;Dispatcher&lt;/code&gt; and call &lt;code&gt;Run()&lt;/code&gt; to create the pool of workers and to start listening for jobs that would appear in the &lt;code&gt;JobQueue&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;dispatcher := NewDispatcher(MaxWorker)
dispatcher.Run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is the code for our dispatcher implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Dispatcher struct {
	// A pool of workers channels that are registered with the dispatcher
	WorkerPool chan chan Job
}

func NewDispatcher(maxWorkers int) *Dispatcher {
	pool := make(chan chan Job, maxWorkers)
	return &amp;amp;Dispatcher{WorkerPool: pool}
}

func (d *Dispatcher) Run() {
    // starting n number of workers
	for i := 0; i &amp;lt; d.maxWorkers; i++ {
		worker := NewWorker(d.pool)
		worker.Start()
	}

	go d.dispatch()
}

func (d *Dispatcher) dispatch() {
	for {
		select {
		case job := &amp;lt;-JobQueue:
			// a job request has been received
			go func(job Job) {
				// try to obtain a worker job channel that is available.
				// this will block until a worker is idle
				jobChannel := &amp;lt;-d.WorkerPool

				// dispatch the job to the worker job channel
				jobChannel &amp;lt;- job
			}(job)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we provide the number of maximum workers to be instantiated and be added to our pool of workers. Since we have utilized Amazon Elasticbeanstalk for this project with a dockerized Go environment, and we always try to follow the &lt;a href=&#34;http://12factor.net/&#34;&gt;12-factor&lt;/a&gt; methodology to configure our systems in production, we read these values from environment variables. That way we could control how many workers and the maximum size of the Job Queue, so we can quickly tweak these values without requiring re-deployment of the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var (
	MaxWorker = os.Getenv(&amp;quot;MAX_WORKERS&amp;quot;)
	MaxQueue  = os.Getenv(&amp;quot;MAX_QUEUE&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-immediate-results:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Immediate results&lt;/h3&gt;

&lt;p&gt;Immediately after we have deployed it we saw all of our latency rates drop to insignificant numbers and our ability to handle requests surged drastically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://marcio.io/img/cloudwatch-console.png&#34; alt=&#34;cloudwatch-console&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Minutes after our Elastic Load Balancers were fully warmed up, we saw our ElasticBeanstalk application serving close to 1 million requests per minute. We usually have a few hours during the morning hours in which our traffic spikes over to more than a million per minute.&lt;/p&gt;

&lt;p&gt;As soon as we have deployed the new code, the number of servers dropped considerably from 100 servers to about 20 servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://marcio.io/img/elasticbeanstalk-healthy-hosts.png&#34; alt=&#34;elasticbeanstalk-healthy-hosts&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After we had properly configured our cluster and the auto-scaling settings, we were able to lower it even more to only 4x EC2 c4.Large instances and the Elastic Auto-Scaling set to spawn a new instance if CPU goes above 90% for 5 minutes straight.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://marcio.io/img/elasticbeanstalk-production-dashboard.png&#34; alt=&#34;elasticbeanstalk-production-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Simplicity always wins in my book. We could have designed a complex system with many queues, background workers, complex deployments, but instead we decided to leverage the power of Elasticbeanstalk auto-scaling and the efficiency and simple approach to concurrency that Golang provides us out of the box.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not everyday that you have a cluster of only 4 machines, that are probably much less powerful than my current MacBook Pro, handling POST requests writing to an Amazon S3 bucket 1 million times every minute.&lt;/p&gt;

&lt;p&gt;There is always the right tool for the job. For sometimes when your Ruby on Rails system needs a very powerful web handler, think a little outside of the ruby eco-system for simpler yet more powerful alternative solutions.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>My First Post</title>
      <link>http://marcio.io/2015/07/my-first-post/</link>
      <pubDate>Mon, 06 Jul 2015 15:57:30 EDT</pubDate>
      <author>Marcio Castilho</author>
      <guid>http://marcio.io/2015/07/my-first-post/</guid>
      <description>&lt;p&gt;For years I have been thinking about creating a spot where I can post articles about programming and technology in general. But every attempt to do so have result in complete failure, mostly due to the hassle of setting up maintaining a blog that can be easily updated. I&amp;rsquo;ve never been a fan of Wordpress and have experimented with other solutions like Octopress and Jekyll, but none of them felt right to me.&lt;/p&gt;

&lt;p&gt;Until I have recently discovered Hugo. It&amp;rsquo;s really awesome and easiy to setup, I can use Markdown to simply create new posts and share my world.&lt;/p&gt;

&lt;p&gt;Check it out at:
&lt;a href=&#34;http://gohugo.io&#34;&gt;http://gohugo.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see if I can keep up with this from now on. Definetely the barrier that was holding me back is gone.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
Marcio&lt;/p&gt;
</description>
    </item>
    
    
  </channel>
</rss>